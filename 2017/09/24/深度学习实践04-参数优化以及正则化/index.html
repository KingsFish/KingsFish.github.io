<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
    

    

    



    <meta charset="utf-8">
    
    
    
    
    <title>深度学习实践04-参数优化以及正则化 | 葫芦籽的碎碎念</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content>
    <meta name="description" content="今天完成了第二门课的第一周内容，主要是三个方面：  偏差和方差的来源，以及减小他们的方法——正则化 梯度爆炸/消失的原因以及解决方法——Xavier随机初始化 梯度检验  下面说一下各自的算法实现 一、偏差以及方差的消除1.1 L2正则化L2正则化主要是在损失函数以及梯度变化量上面进行修改，损失函数修改之后的函数如下：">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习实践04-参数优化以及正则化">
<meta property="og:url" content="https://KingsFish.github.io/2017/09/24/深度学习实践04-参数优化以及正则化/index.html">
<meta property="og:site_name" content="葫芦籽的碎碎念">
<meta property="og:description" content="今天完成了第二门课的第一周内容，主要是三个方面：  偏差和方差的来源，以及减小他们的方法——正则化 梯度爆炸/消失的原因以及解决方法——Xavier随机初始化 梯度检验  下面说一下各自的算法实现 一、偏差以及方差的消除1.1 L2正则化L2正则化主要是在损失函数以及梯度变化量上面进行修改，损失函数修改之后的函数如下：">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://kingsfish.github.io/2017/09/24/深度学习实践04-参数优化以及正则化/cost.jpg">
<meta property="og:image" content="https://kingsfish.github.io/2017/09/24/深度学习实践04-参数优化以及正则化/dw.jpg">
<meta property="og:image" content="https://kingsfish.github.io/2017/09/24/深度学习实践04-参数优化以及正则化/xva.jpg">
<meta property="og:image" content="https://kingsfish.github.io/2017/09/24/深度学习实践04-参数优化以及正则化/approx.jpg">
<meta property="og:image" content="https://kingsfish.github.io/2017/09/24/深度学习实践04-参数优化以及正则化/diff.jpg">
<meta property="og:updated_time" content="2019-06-14T14:54:54.995Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="深度学习实践04-参数优化以及正则化">
<meta name="twitter:description" content="今天完成了第二门课的第一周内容，主要是三个方面：  偏差和方差的来源，以及减小他们的方法——正则化 梯度爆炸/消失的原因以及解决方法——Xavier随机初始化 梯度检验  下面说一下各自的算法实现 一、偏差以及方差的消除1.1 L2正则化L2正则化主要是在损失函数以及梯度变化量上面进行修改，损失函数修改之后的函数如下：">
<meta name="twitter:image" content="https://kingsfish.github.io/2017/09/24/深度学习实践04-参数优化以及正则化/cost.jpg">
    
        <link rel="alternate" type="application/atom+xml" title="葫芦籽的碎碎念" href="/atom.xml">
    
    <link rel="shortcut icon" href="/favicon.ico">
    <link rel="stylesheet" href="//unpkg.com/hexo-theme-material-indigo@latest/css/style.css">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

</head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide">
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="/about" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/logo.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">Fish</h5>
          <a href="mailto:kingsfish95@gmail.com" title="kingsfish95@gmail.com" class="mail">kingsfish95@gmail.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/">
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives">
                <i class="icon icon-lg icon-archives"></i>
                归档
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags">
                <i class="icon icon-lg icon-tags"></i>
                标签
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories">
                <i class="icon icon-lg icon-th-list"></i>
                分类
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/KingsFish" target="_blank">
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/about">
                <i class="icon icon-lg icon-user-o"></i>
                关于
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/atom.xml">
                <i class="icon icon-lg icon-rss"></i>
                RSS订阅
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">深度学习实践04-参数优化以及正则化</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="输入感兴趣的关键字">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">深度学习实践04-参数优化以及正则化</h1>
        <h5 class="subtitle">
            
                <time datetime="2017-09-24T14:23:50.000Z" itemprop="datePublished" class="page-time">
  2017-09-24
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/机器学习/">机器学习</a></li></ul>

            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap post-toc-shrink" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#一、偏差以及方差的消除"><span class="post-toc-number">1.</span> <span class="post-toc-text">一、偏差以及方差的消除</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#1-1-L2正则化"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">1.1 L2正则化</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#1-2-dropout正则化"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">1.2 dropout正则化</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#二、梯度爆炸-消失的原因以及解决方法"><span class="post-toc-number">2.</span> <span class="post-toc-text">二、梯度爆炸/消失的原因以及解决方法</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#三、梯度检验"><span class="post-toc-number">3.</span> <span class="post-toc-text">三、梯度检验</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#总结"><span class="post-toc-number">4.</span> <span class="post-toc-text">总结</span></a></li></ol>
        </nav>
    </aside>


<article id="post-深度学习实践04-参数优化以及正则化" class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">深度学习实践04-参数优化以及正则化</h1>
        <div class="post-meta">
            <time class="post-time" title="2017-09-24 22:23:50" datetime="2017-09-24T14:23:50.000Z" itemprop="datePublished">2017-09-24</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/机器学习/">机器学习</a></li></ul>



            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style="display:none">
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <p>今天完成了第二门课的第一周内容，主要是三个方面：</p>
<ol>
<li>偏差和方差的来源，以及减小他们的方法——正则化</li>
<li>梯度爆炸/消失的原因以及解决方法——Xavier随机初始化</li>
<li>梯度检验</li>
</ol>
<p>下面说一下各自的算法实现</p>
<h1 id="一、偏差以及方差的消除"><a href="#一、偏差以及方差的消除" class="headerlink" title="一、偏差以及方差的消除"></a>一、偏差以及方差的消除</h1><h2 id="1-1-L2正则化"><a href="#1-1-L2正则化" class="headerlink" title="1.1 L2正则化"></a>1.1 L2正则化</h2><p><strong>L2正则化</strong>主要是在损失函数以及梯度变化量上面进行修改，损失函数修改之后的函数如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="cost.jpg" alt="损失函数" title>
                </div>
                <div class="image-caption">损失函数</div>
            </figure>
<p>梯度变化量修改之后如下</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="dw.jpg" alt="梯度变换量" title>
                </div>
                <div class="image-caption">梯度变换量</div>
            </figure> 

<p>先实现成本函数的计算，该神经网络是一个3层的神经网络</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost_with_regularization</span><span class="params">(A3, Y, parameters, lambd)</span>:</span></span><br><span class="line">    m = Y.shape[<span class="number">1</span>]</span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    W3 = parameters[<span class="string">"W3"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute_cost函数就是之前的成本函数</span></span><br><span class="line">    cross_entropy_cost = compute_cost(A3, Y)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算L2正则化</span></span><br><span class="line">    L2_regularization_cost = lambd * (np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3))) / (<span class="number">2</span> * m)</span><br><span class="line">    </span><br><span class="line">    cost = cross_entropy_cost + L2_regularization_cost</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure>

<p>然后是梯度变化量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation_with_regularization</span><span class="params">(X, Y, cache, lambd)</span>:</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache</span><br><span class="line">    </span><br><span class="line">    dZ3 = A3 - Y</span><br><span class="line">    <span class="comment"># lambd * W3 / m 就是L2正则化所添加的项目，以下同理</span></span><br><span class="line">    dW3 = <span class="number">1.</span>/m * np.dot(dZ3, A2.T) + lambd * W3 / m</span><br><span class="line">    db3 = <span class="number">1.</span>/m * np.sum(dZ3, axis=<span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    dA2 = np.dot(W3.T, dZ3)</span><br><span class="line">    dZ2 = np.multiply(dA2, np.int64(A2 &gt; <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    dW2 = <span class="number">1.</span>/m * np.dot(dZ2, A1.T) + lambd * W2 / m</span><br><span class="line">    db2 = <span class="number">1.</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">    dZ1 = np.multiply(dA1, np.int64(A1 &gt; <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    dW1 = <span class="number">1.</span>/m * np.dot(dZ1, X.T) + lambd * W1 / m</span><br><span class="line">    db1 = <span class="number">1.</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    gradients = &#123;<span class="string">"dZ3"</span>: dZ3, <span class="string">"dW3"</span>: dW3, <span class="string">"db3"</span>: db3,<span class="string">"dA2"</span>: dA2,</span><br><span class="line">                 <span class="string">"dZ2"</span>: dZ2, <span class="string">"dW2"</span>: dW2, <span class="string">"db2"</span>: db2, <span class="string">"dA1"</span>: dA1, </span><br><span class="line">                 <span class="string">"dZ1"</span>: dZ1, <span class="string">"dW1"</span>: dW1, <span class="string">"db1"</span>: db1&#125;</span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure>

<h2 id="1-2-dropout正则化"><a href="#1-2-dropout正则化" class="headerlink" title="1.2 dropout正则化"></a>1.2 dropout正则化</h2><p><strong>dropout正则化</strong>的主要思路是随机消除每一层中的某一些神经单元，以此来减少对输入的以来，从来减轻过拟合——也即方差过大的问题。使用的方法流程如下</p>
<ol>
<li>随机初始化一个和每一层的输出<code>A</code>矩阵同纬度的矩阵<code>D</code></li>
<li>根据阈值<code>keep_prob</code>将<code>D</code>中的元素变成0和1，然后再与<code>A</code>相乘来消除<code>A</code>中某些层中的某一些节点</li>
<li>将<code>A</code>中的均值进行复原</li>
</ol>
<p>同时，<code>dA</code>也需要进行相同的操作，流程与上面的一致。需要注意的是，我们不对输入和输出层进行<strong>dropout正则化</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation_with_dropout</span><span class="params">(X, parameters, keep_prob = <span class="number">0.5</span>)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># retrieve parameters</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    W3 = parameters[<span class="string">"W3"</span>]</span><br><span class="line">    b3 = parameters[<span class="string">"b3"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID</span></span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = relu(Z1) </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对第一层进行dropout正则化</span></span><br><span class="line">    D1 = np.random.rand(A1.shape[<span class="number">0</span>], A1.shape[<span class="number">1</span>]) </span><br><span class="line">    D1 = (D1 &lt;= keep_prob)</span><br><span class="line">    A1 = A1 * D1   </span><br><span class="line">    A1 = A1 / keep_prob </span><br><span class="line">    </span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = relu(Z2)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对第二层进行dropout正则化</span></span><br><span class="line">    D2 = np.random.rand(A2.shape[<span class="number">0</span>], A2.shape[<span class="number">1</span>])</span><br><span class="line">    D2 = (D2 &lt;= keep_prob)</span><br><span class="line">    A2 = A2 * D2</span><br><span class="line">    A2 = A2 / keep_prob</span><br><span class="line">    </span><br><span class="line">    Z3 = np.dot(W3, A2) + b3</span><br><span class="line">    A3 = sigmoid(Z3)</span><br><span class="line">    </span><br><span class="line">    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)    </span><br><span class="line">    <span class="keyword">return</span> A3, cache</span><br></pre></td></tr></table></figure>

<p>其次是对<code>dA</code>进行相同操作，需要注意的是<code>dA1</code>和<code>A1</code>使用的应该是相同的随机矩阵<code>D1</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation_with_dropout</span><span class="params">(X, Y, cache, keep_prob)</span>:</span>   </span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache</span><br><span class="line">    </span><br><span class="line">    dZ3 = A3 - Y</span><br><span class="line">    dW3 = <span class="number">1.</span>/m * np.dot(dZ3, A2.T)</span><br><span class="line">    db3 = <span class="number">1.</span>/m * np.sum(dZ3, axis=<span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line">    dA2 = np.dot(W3.T, dZ3)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对dA2进行dropout正则化</span></span><br><span class="line">    dA2 = dA2 * D2</span><br><span class="line">    dA2 = dA2 / keep_prob</span><br><span class="line"></span><br><span class="line">    dZ2 = np.multiply(dA2, np.int64(A2 &gt; <span class="number">0</span>))</span><br><span class="line">    dW2 = <span class="number">1.</span>/m * np.dot(dZ2, A1.T)</span><br><span class="line">    db2 = <span class="number">1.</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line">    dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对dA1进行dropout正则化</span></span><br><span class="line">    dA1 = dA1 * D1</span><br><span class="line">    dA1 = dA1 / keep_prob</span><br><span class="line">    </span><br><span class="line">    dZ1 = np.multiply(dA1, np.int64(A1 &gt; <span class="number">0</span>))</span><br><span class="line">    dW1 = <span class="number">1.</span>/m * np.dot(dZ1, X.T)</span><br><span class="line">    db1 = <span class="number">1.</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    gradients = &#123;<span class="string">"dZ3"</span>: dZ3, <span class="string">"dW3"</span>: dW3, <span class="string">"db3"</span>: db3,<span class="string">"dA2"</span>: dA2,</span><br><span class="line">                 <span class="string">"dZ2"</span>: dZ2, <span class="string">"dW2"</span>: dW2, <span class="string">"db2"</span>: db2, <span class="string">"dA1"</span>: dA1, </span><br><span class="line">                 <span class="string">"dZ1"</span>: dZ1, <span class="string">"dW1"</span>: dW1, <span class="string">"db1"</span>: db1&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure>

<h1 id="二、梯度爆炸-消失的原因以及解决方法"><a href="#二、梯度爆炸-消失的原因以及解决方法" class="headerlink" title="二、梯度爆炸/消失的原因以及解决方法"></a>二、梯度爆炸/消失的原因以及解决方法</h1><p>梯度爆炸以及消失的主要是由<code>W</code>的值以及网络深度导致的，在比较深的网络里面，<code>W</code>增长速度是幂级别的，而<code>Z=WX+b</code>，这样很容易导致<code>Z</code>过大导致梯度变化量过小而降低学习速度。一个常用的解决方法是<code>Xavier初始化</code>，公式如下</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="xva.jpg" alt="初始化公式" title>
                </div>
                <div class="image-caption">初始化公式</div>
            </figure>

<p>这个方法实现起来比较简单。只需要在初始化<code>W</code>的时候乘上一个因子即可，常用的激活函数是<code>Relu</code>函数，<code>tanh</code>函数与其类似，因此只实现<code>Relu</code>函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_he</span><span class="params">(layers_dims)</span>:</span></span><br><span class="line"></span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layers_dims) - <span class="number">1</span> </span><br><span class="line">     </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L + <span class="number">1</span>):</span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.random.randn(layers_dims[l], layers_dims[l - <span class="number">1</span>]) * np.sqrt(<span class="number">2</span> / layers_dims[l - <span class="number">1</span>])</span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layers_dims[l], <span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>

<h1 id="三、梯度检验"><a href="#三、梯度检验" class="headerlink" title="三、梯度检验"></a>三、梯度检验</h1><p>梯度检验的目的是确认反向传播的正确性，因为反向传播较为复杂，很容易出现不知名bug。梯度检验的流程主要如下：</p>
<ol>
<li>将<code>W</code>和<code>b</code>合成一个新的变量<code>θ</code></li>
<li>利用公式计算<code>θ</code>近似值<br><img src="approx.jpg" alt="近似值计算公式"></li>
<li>根据反向传播计算<code>W</code>和<code>b</code>并合成为新的的变量<code>θ</code></li>
<li>计算<code>θ</code>近似值与真实值之间的差距，如果小于某一个数，则认为计算正确，否则就需要检验所有的反向计算是否出错<br><img src="diff.jpg" alt="差距计算公式"></li>
</ol>
<p>由于梯度检验的复杂性，本文实现的是较为简单的梯度检验算法<code>J=θx</code>。需要注意的是，梯度检验只能用于debug，不能用于训练，因为梯度检验需要的时间很长。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_check</span><span class="params">(x, theta, epsilon = <span class="number">1e-7</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算近似值</span></span><br><span class="line">    thetaplus = theta + epsilon                             </span><br><span class="line">    thetaminus = theta - epsilon</span><br><span class="line">    J_plus = x * thetaplus</span><br><span class="line">    J_minus = x * thetaminus</span><br><span class="line">    gradapprox = (J_plus - J_minus) / (<span class="number">2</span> * epsilon)</span><br><span class="line">    </span><br><span class="line">    grad = x</span><br><span class="line"></span><br><span class="line">    numerator = np.linalg.norm(grad - gradapprox)</span><br><span class="line">    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)       </span><br><span class="line">    difference = numerator / denominator</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> difference &lt; <span class="number">1e-7</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"The gradient is correct!"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"The gradient is wrong!"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> difference</span><br></pre></td></tr></table></figure>

<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>这周的课程主要是对神经网络的优化，包括Xavier初始化，以及如何debug等，以加快机器学习的速度，减少错误率。下周将学习全新的优化算法——MiniBatch梯度下降法。</p>

        </div>

        <blockquote class="post-copyright">
    
    <div class="content">
        
<span class="post-time">
    最后更新时间：<time datetime="2019-06-14T14:54:54.995Z" itemprop="dateUpdated">2019-06-14 22:54:54</time>
</span><br>


        
        原文链接：<a href="/2017/09/24/深度学习实践04-参数优化以及正则化/" target="_blank" rel="external">https://KingsFish.github.io/2017/09/24/深度学习实践04-参数优化以及正则化/</a>
        
    </div>
    
    <footer>
        <a href="https://KingsFish.github.io">
            <img src="/img/logo.jpg" alt="Fish">
            Fish
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            

            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://KingsFish.github.io/2017/09/24/深度学习实践04-参数优化以及正则化/&title=《深度学习实践04-参数优化以及正则化》 — 葫芦籽的碎碎念&pic=https://KingsFish.github.io/img/logo.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://KingsFish.github.io/2017/09/24/深度学习实践04-参数优化以及正则化/&title=《深度学习实践04-参数优化以及正则化》 — 葫芦籽的碎碎念&source=talk is cheap, show me the code" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://KingsFish.github.io/2017/09/24/深度学习实践04-参数优化以及正则化/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《深度学习实践04-参数优化以及正则化》 — 葫芦籽的碎碎念&url=https://KingsFish.github.io/2017/09/24/深度学习实践04-参数优化以及正则化/&via=https://KingsFish.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://KingsFish.github.io/2017/09/24/深度学习实践04-参数优化以及正则化/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/2017/09/29/深度学习实践05-优化算法/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">深度学习实践05-优化算法</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/2017/09/24/深度学习实践03-构建深层神经网络/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">深度学习实践03-构建深层神经网络</h4>
      </a>
    </div>
  
</nav>



    











    <!-- Valine Comments -->
    <div class="comments vcomment" id="comments"></div>
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script>
    <!-- Valine Comments script -->
    <script>
        var GUEST_INFO = ['nick','mail','link'];
        var guest_info = 'nick,mail,link'.split(',').filter(function(item){
          return GUEST_INFO.indexOf(item) > -1
        });
        new Valine({
            el: '#comments',
            notify: 'true' == 'true',
            verify: 'false' == 'true',
            appId: "2CfM2Fm7VsbIigabgehi8g05-gzGzoHsz",
            appKey: "dgmqDkxfsXHQlooYvAtcN6ja",
            avatar: "retro",
            placeholder: "来说点什么吧~记得留下邮箱，这样有提醒哦",
            guest_info: guest_info.length == 0 ? GUEST_INFO : guest_info,
            pageSize: "10"
        })
    </script>
    <!-- Valine Comments end -->










</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        打赏点饭钱~
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/img/wechat.jpg" alt="打赏二维码">
        </div>
        
        <label class="reward-toggle">
            <input id="rewardToggle" type="checkbox" class="reward-toggle-check" data-wechat="/img/wechat.jpg" data-alipay="/img/alipay.jpg">
            <div class="reward-toggle-ctrol">
                <span class="reward-toggle-item wechat">微信</span>
                <span class="reward-toggle-label"></span>
                <span class="reward-toggle-item alipay">支付宝</span>
            </div>
        </label>
        
    </div>
</div>



</div>

        <footer class="footer">
    <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style="display:none">
        站点总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style="display:none">
        站点总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>博客内容遵循 <a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">知识共享 署名 - 非商业性 - 相同方式共享 4.0 国际协议</a></span>
        </p>
    </div>
    <div class="bottom">
        <p><span>Fish &copy; 2017 - 2020</span>
            <span>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://KingsFish.github.io/2017/09/24/深度学习实践04-参数优化以及正则化/&title=《深度学习实践04-参数优化以及正则化》 — 葫芦籽的碎碎念&pic=https://KingsFish.github.io/img/logo.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://KingsFish.github.io/2017/09/24/深度学习实践04-参数优化以及正则化/&title=《深度学习实践04-参数优化以及正则化》 — 葫芦籽的碎碎念&source=talk is cheap, show me the code" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://KingsFish.github.io/2017/09/24/深度学习实践04-参数优化以及正则化/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《深度学习实践04-参数优化以及正则化》 — 葫芦籽的碎碎念&url=https://KingsFish.github.io/2017/09/24/深度学习实践04-参数优化以及正则化/&via=https://KingsFish.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://KingsFish.github.io/2017/09/24/深度学习实践04-参数优化以及正则化/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAPYAAAD2CAAAAADAeSUUAAADKUlEQVR42u3aS27bQBAFQN3/0gqQXeBIeq+HBsx2zcqgKXFquGj15/GI1/Pv+nrl1fVX9zxfrOT+V0//+t/LFjY2NvZN2M+36z2m3WhyiO0RJ3v7z9FgY2Njr2O/D1rJQeSHMosy7U4+GLGxsbF/JTvZUB6Q8iOeHQE2NjY29uwBeaCalZbaJAcbGxv797DzUlGbkJy0E/JGxTfW0rCxsbF/PLtt9P7kv7+xv42NjY39I9nPcs0KRrMGQ/saCgU2Njb2Inby474dnZklM21juC1I1TkNNjY29s3Z54M759/ffqoOhNjY2Ngr2CfpxCxFaVu2bfshajNgY2NjL2W3X9GGq/x6PkI0ezHY2NjYu9ltcScf0GmfcpLkfLiOjY2NvY59beCZrTadOEpLsLGxsRex2yL+jNremQ/rJC/ggxEbGxv75ux8o8lxFL3lsp2QH0S0f2xsbOx17BnyEa98+KYNSPnxYWNjY29lJ0Ho2obBLC05bye8rKhhY2Njr2C3gWSWBrTthyQ45a+tqKVhY2Nj35Cdf10SGK5tCcxKTtFTsLGxsRex27JRC2iHftqyV5uQYGNjY29inzdxkxByfrgJsrgHGxsbexG7DScn5Z48EF41BhqFUmxsbOwV7PPCTXKlLUjNSkv1e8bGxsa+OfvaoZl263l7YJYgfRjTwcbGxl7EzsNSOxYzG45ss6j6CjY2NvYidl6sb8NYG4TyNKZtCbw8SmxsbOwV7Pzx7SNngfCkZVsMYmJjY2OvZj+CVZRv4nSlbTbkRxPV0rCxsbGXsmeH8h6WbK5tANRNCGxsbOxF7NnP/bb8NAtIRWoxsmBjY2PvYCeMk3DSpivJZy8IY9jY2Njr2OeBof1UW346L379cwUbGxt7HTtfs7JRG9jaoJXvM5oYwsbGxr4Ve1bubxOJth07S1dOvhkbGxv77ux2BPOqQ2mLSrMAFnUwsLGxsVewk8CQjPWcj9pcBsPGxsbGDrZ+Vev3hN2Wn7CxsbGx2yCXh7rZiM+sOYGNjY29j92GkDw9mB3HdzchsLGxsTexzxu97x8zazPM2hWzg8PGxsa+LfsPvP4RJKtHSJ0AAAAASUVORK5CYII=" alt="微信分享二维码">
</div>




    <script src="//cdnjs.cloudflare.com/ajax/libs/node-waves/0.7.6/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };


</script>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/main.min.js"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/search.min.js" async></script>






<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>



<script>
(function() {
    var OriginTitile = document.title, titleTime;
    document.addEventListener('visibilitychange', function() {
        if (document.hidden) {
            document.title = '死鬼去哪里了！';
            clearTimeout(titleTime);
        } else {
            document.title = '(つェ⊂)咦!又好了!';
            titleTime = setTimeout(function() {
                document.title = OriginTitile;
            },2000);
        }
    });
})();
</script>



</body>
</html>
