<!DOCTYPE html>
<html>
<head>
    

    

    



    <meta charset="utf-8">
    
    <meta name="google-site-verification" content="true">
    
    
    
    
    <title>深度学习实践04-参数优化以及正则化 | 葫芦籽的碎碎念</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="">
    <meta name="description" content="今天完成了第二门课的第一周内容，主要是三个方面：  偏差和方差的来源，以及减小他们的方法——正则化 梯度爆炸&#x2F;消失的原因以及解决方法——Xavier随机初始化 梯度检验  下面说一下各自的算法实现 一、偏差以及方差的消除1.1 L2正则化L2正则化主要是在损失函数以及梯度变化量上面进行修改，损失函数修改之后的函数如下：">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习实践04-参数优化以及正则化">
<meta property="og:url" content="https://kingsfish.github.io/2017/09/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B504-%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96%E4%BB%A5%E5%8F%8A%E6%AD%A3%E5%88%99%E5%8C%96/index.html">
<meta property="og:site_name" content="葫芦籽的碎碎念">
<meta property="og:description" content="今天完成了第二门课的第一周内容，主要是三个方面：  偏差和方差的来源，以及减小他们的方法——正则化 梯度爆炸&#x2F;消失的原因以及解决方法——Xavier随机初始化 梯度检验  下面说一下各自的算法实现 一、偏差以及方差的消除1.1 L2正则化L2正则化主要是在损失函数以及梯度变化量上面进行修改，损失函数修改之后的函数如下：">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://kingsfish.github.io/2017/09/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B504-%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96%E4%BB%A5%E5%8F%8A%E6%AD%A3%E5%88%99%E5%8C%96/cost.jpg">
<meta property="og:image" content="https://kingsfish.github.io/2017/09/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B504-%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96%E4%BB%A5%E5%8F%8A%E6%AD%A3%E5%88%99%E5%8C%96/dw.jpg">
<meta property="og:image" content="https://kingsfish.github.io/2017/09/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B504-%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96%E4%BB%A5%E5%8F%8A%E6%AD%A3%E5%88%99%E5%8C%96/xva.jpg">
<meta property="og:image" content="https://kingsfish.github.io/approx.jpg">
<meta property="og:image" content="https://kingsfish.github.io/diff.jpg">
<meta property="article:published_time" content="2017-09-24T14:23:50.000Z">
<meta property="article:modified_time" content="2019-06-14T14:54:54.995Z">
<meta property="article:author" content="Fish">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://kingsfish.github.io/2017/09/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B504-%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96%E4%BB%A5%E5%8F%8A%E6%AD%A3%E5%88%99%E5%8C%96/cost.jpg">
    
        <link rel="alternate" type="application/atom+xml" title="葫芦籽的碎碎念" href="/atom.xml">
    
    <link rel="shortcut icon" href="/favicon.ico">
    <link rel="stylesheet" href="/css/style.css?v=1.7.2">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

<meta name="generator" content="Hexo 6.2.0"></head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="/about" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/logo.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">Fish</h5>
          <a href="mailto:kingsfish95@gmail.com" title="kingsfish95@gmail.com" class="mail">kingsfish95@gmail.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                归档
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                标签
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories"  >
                <i class="icon icon-lg icon-th-list"></i>
                分类
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/KingsFish" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/about"  >
                <i class="icon icon-lg icon-user-o"></i>
                关于
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/atom.xml"  >
                <i class="icon icon-lg icon-rss"></i>
                RSS订阅
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">深度学习实践04-参数优化以及正则化</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="输入感兴趣的关键字">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">深度学习实践04-参数优化以及正则化</h1>
        <h5 class="subtitle">
            
                <time datetime="2017-09-24T14:23:50.000Z" itemprop="datePublished" class="page-time">
  2017-09-24
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li></ul>

            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap post-toc-shrink" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#%E4%B8%80%E3%80%81%E5%81%8F%E5%B7%AE%E4%BB%A5%E5%8F%8A%E6%96%B9%E5%B7%AE%E7%9A%84%E6%B6%88%E9%99%A4"><span class="post-toc-number">1.</span> <span class="post-toc-text">一、偏差以及方差的消除</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#1-1-L2%E6%AD%A3%E5%88%99%E5%8C%96"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">1.1 L2正则化</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#1-2-dropout%E6%AD%A3%E5%88%99%E5%8C%96"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">1.2 dropout正则化</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#%E4%BA%8C%E3%80%81%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8-x2F-%E6%B6%88%E5%A4%B1%E7%9A%84%E5%8E%9F%E5%9B%A0%E4%BB%A5%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95"><span class="post-toc-number">2.</span> <span class="post-toc-text">二、梯度爆炸&#x2F;消失的原因以及解决方法</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#%E4%B8%89%E3%80%81%E6%A2%AF%E5%BA%A6%E6%A3%80%E9%AA%8C"><span class="post-toc-number">3.</span> <span class="post-toc-text">三、梯度检验</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#%E6%80%BB%E7%BB%93"><span class="post-toc-number">4.</span> <span class="post-toc-text">总结</span></a></li></ol>
        </nav>
    </aside>


<article id="post-深度学习实践04-参数优化以及正则化"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">深度学习实践04-参数优化以及正则化</h1>
        <div class="post-meta">
            <time class="post-time" title="2017-09-24 22:23:50" datetime="2017-09-24T14:23:50.000Z"  itemprop="datePublished">2017-09-24</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li></ul>



            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <p>今天完成了第二门课的第一周内容，主要是三个方面：</p>
<ol>
<li>偏差和方差的来源，以及减小他们的方法——正则化</li>
<li>梯度爆炸&#x2F;消失的原因以及解决方法——Xavier随机初始化</li>
<li>梯度检验</li>
</ol>
<p>下面说一下各自的算法实现</p>
<h1 id="一、偏差以及方差的消除"><a href="#一、偏差以及方差的消除" class="headerlink" title="一、偏差以及方差的消除"></a>一、偏差以及方差的消除</h1><h2 id="1-1-L2正则化"><a href="#1-1-L2正则化" class="headerlink" title="1.1 L2正则化"></a>1.1 L2正则化</h2><p><strong>L2正则化</strong>主要是在损失函数以及梯度变化量上面进行修改，损失函数修改之后的函数如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="cost.jpg" alt="损失函数" title="">
                </div>
                <div class="image-caption">损失函数</div>
            </figure>
<p>梯度变化量修改之后如下</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="dw.jpg" alt="梯度变换量" title="">
                </div>
                <div class="image-caption">梯度变换量</div>
            </figure> 

<p>先实现成本函数的计算，该神经网络是一个3层的神经网络</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_cost_with_regularization</span>(<span class="params">A3, Y, parameters, lambd</span>):</span><br><span class="line">    m = Y.shape[<span class="number">1</span>]</span><br><span class="line">    W1 = parameters[<span class="string">&quot;W1&quot;</span>]</span><br><span class="line">    W2 = parameters[<span class="string">&quot;W2&quot;</span>]</span><br><span class="line">    W3 = parameters[<span class="string">&quot;W3&quot;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute_cost函数就是之前的成本函数</span></span><br><span class="line">    cross_entropy_cost = compute_cost(A3, Y)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算L2正则化</span></span><br><span class="line">    L2_regularization_cost = lambd * (np.<span class="built_in">sum</span>(np.square(W1)) + np.<span class="built_in">sum</span>(np.square(W2)) + np.<span class="built_in">sum</span>(np.square(W3))) / (<span class="number">2</span> * m)</span><br><span class="line">    </span><br><span class="line">    cost = cross_entropy_cost + L2_regularization_cost</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure>

<p>然后是梯度变化量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">backward_propagation_with_regularization</span>(<span class="params">X, Y, cache, lambd</span>):</span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache</span><br><span class="line">    </span><br><span class="line">    dZ3 = A3 - Y</span><br><span class="line">    <span class="comment"># lambd * W3 / m 就是L2正则化所添加的项目，以下同理</span></span><br><span class="line">    dW3 = <span class="number">1.</span>/m * np.dot(dZ3, A2.T) + lambd * W3 / m</span><br><span class="line">    db3 = <span class="number">1.</span>/m * np.<span class="built_in">sum</span>(dZ3, axis=<span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    dA2 = np.dot(W3.T, dZ3)</span><br><span class="line">    dZ2 = np.multiply(dA2, np.int64(A2 &gt; <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    dW2 = <span class="number">1.</span>/m * np.dot(dZ2, A1.T) + lambd * W2 / m</span><br><span class="line">    db2 = <span class="number">1.</span>/m * np.<span class="built_in">sum</span>(dZ2, axis=<span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">    dZ1 = np.multiply(dA1, np.int64(A1 &gt; <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    dW1 = <span class="number">1.</span>/m * np.dot(dZ1, X.T) + lambd * W1 / m</span><br><span class="line">    db1 = <span class="number">1.</span>/m * np.<span class="built_in">sum</span>(dZ1, axis=<span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    gradients = &#123;<span class="string">&quot;dZ3&quot;</span>: dZ3, <span class="string">&quot;dW3&quot;</span>: dW3, <span class="string">&quot;db3&quot;</span>: db3,<span class="string">&quot;dA2&quot;</span>: dA2,</span><br><span class="line">                 <span class="string">&quot;dZ2&quot;</span>: dZ2, <span class="string">&quot;dW2&quot;</span>: dW2, <span class="string">&quot;db2&quot;</span>: db2, <span class="string">&quot;dA1&quot;</span>: dA1, </span><br><span class="line">                 <span class="string">&quot;dZ1&quot;</span>: dZ1, <span class="string">&quot;dW1&quot;</span>: dW1, <span class="string">&quot;db1&quot;</span>: db1&#125;</span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure>

<h2 id="1-2-dropout正则化"><a href="#1-2-dropout正则化" class="headerlink" title="1.2 dropout正则化"></a>1.2 dropout正则化</h2><p><strong>dropout正则化</strong>的主要思路是随机消除每一层中的某一些神经单元，以此来减少对输入的以来，从来减轻过拟合——也即方差过大的问题。使用的方法流程如下</p>
<ol>
<li>随机初始化一个和每一层的输出<code>A</code>矩阵同纬度的矩阵<code>D</code></li>
<li>根据阈值<code>keep_prob</code>将<code>D</code>中的元素变成0和1，然后再与<code>A</code>相乘来消除<code>A</code>中某些层中的某一些节点</li>
<li>将<code>A</code>中的均值进行复原</li>
</ol>
<p>同时，<code>dA</code>也需要进行相同的操作，流程与上面的一致。需要注意的是，我们不对输入和输出层进行<strong>dropout正则化</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward_propagation_with_dropout</span>(<span class="params">X, parameters, keep_prob = <span class="number">0.5</span></span>):</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># retrieve parameters</span></span><br><span class="line">    W1 = parameters[<span class="string">&quot;W1&quot;</span>]</span><br><span class="line">    b1 = parameters[<span class="string">&quot;b1&quot;</span>]</span><br><span class="line">    W2 = parameters[<span class="string">&quot;W2&quot;</span>]</span><br><span class="line">    b2 = parameters[<span class="string">&quot;b2&quot;</span>]</span><br><span class="line">    W3 = parameters[<span class="string">&quot;W3&quot;</span>]</span><br><span class="line">    b3 = parameters[<span class="string">&quot;b3&quot;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID</span></span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = relu(Z1) </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对第一层进行dropout正则化</span></span><br><span class="line">    D1 = np.random.rand(A1.shape[<span class="number">0</span>], A1.shape[<span class="number">1</span>]) </span><br><span class="line">    D1 = (D1 &lt;= keep_prob)</span><br><span class="line">    A1 = A1 * D1   </span><br><span class="line">    A1 = A1 / keep_prob </span><br><span class="line">    </span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = relu(Z2)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对第二层进行dropout正则化</span></span><br><span class="line">    D2 = np.random.rand(A2.shape[<span class="number">0</span>], A2.shape[<span class="number">1</span>])</span><br><span class="line">    D2 = (D2 &lt;= keep_prob)</span><br><span class="line">    A2 = A2 * D2</span><br><span class="line">    A2 = A2 / keep_prob</span><br><span class="line">    </span><br><span class="line">    Z3 = np.dot(W3, A2) + b3</span><br><span class="line">    A3 = sigmoid(Z3)</span><br><span class="line">    </span><br><span class="line">    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)    </span><br><span class="line">    <span class="keyword">return</span> A3, cache</span><br></pre></td></tr></table></figure>

<p>其次是对<code>dA</code>进行相同操作，需要注意的是<code>dA1</code>和<code>A1</code>使用的应该是相同的随机矩阵<code>D1</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">backward_propagation_with_dropout</span>(<span class="params">X, Y, cache, keep_prob</span>):   </span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache</span><br><span class="line">    </span><br><span class="line">    dZ3 = A3 - Y</span><br><span class="line">    dW3 = <span class="number">1.</span>/m * np.dot(dZ3, A2.T)</span><br><span class="line">    db3 = <span class="number">1.</span>/m * np.<span class="built_in">sum</span>(dZ3, axis=<span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line">    dA2 = np.dot(W3.T, dZ3)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对dA2进行dropout正则化</span></span><br><span class="line">    dA2 = dA2 * D2</span><br><span class="line">    dA2 = dA2 / keep_prob</span><br><span class="line"></span><br><span class="line">    dZ2 = np.multiply(dA2, np.int64(A2 &gt; <span class="number">0</span>))</span><br><span class="line">    dW2 = <span class="number">1.</span>/m * np.dot(dZ2, A1.T)</span><br><span class="line">    db2 = <span class="number">1.</span>/m * np.<span class="built_in">sum</span>(dZ2, axis=<span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line">    dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对dA1进行dropout正则化</span></span><br><span class="line">    dA1 = dA1 * D1</span><br><span class="line">    dA1 = dA1 / keep_prob</span><br><span class="line">    </span><br><span class="line">    dZ1 = np.multiply(dA1, np.int64(A1 &gt; <span class="number">0</span>))</span><br><span class="line">    dW1 = <span class="number">1.</span>/m * np.dot(dZ1, X.T)</span><br><span class="line">    db1 = <span class="number">1.</span>/m * np.<span class="built_in">sum</span>(dZ1, axis=<span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    gradients = &#123;<span class="string">&quot;dZ3&quot;</span>: dZ3, <span class="string">&quot;dW3&quot;</span>: dW3, <span class="string">&quot;db3&quot;</span>: db3,<span class="string">&quot;dA2&quot;</span>: dA2,</span><br><span class="line">                 <span class="string">&quot;dZ2&quot;</span>: dZ2, <span class="string">&quot;dW2&quot;</span>: dW2, <span class="string">&quot;db2&quot;</span>: db2, <span class="string">&quot;dA1&quot;</span>: dA1, </span><br><span class="line">                 <span class="string">&quot;dZ1&quot;</span>: dZ1, <span class="string">&quot;dW1&quot;</span>: dW1, <span class="string">&quot;db1&quot;</span>: db1&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure>

<h1 id="二、梯度爆炸-x2F-消失的原因以及解决方法"><a href="#二、梯度爆炸-x2F-消失的原因以及解决方法" class="headerlink" title="二、梯度爆炸&#x2F;消失的原因以及解决方法"></a>二、梯度爆炸&#x2F;消失的原因以及解决方法</h1><p>梯度爆炸以及消失的主要是由<code>W</code>的值以及网络深度导致的，在比较深的网络里面，<code>W</code>增长速度是幂级别的，而<code>Z=WX+b</code>，这样很容易导致<code>Z</code>过大导致梯度变化量过小而降低学习速度。一个常用的解决方法是<code>Xavier初始化</code>，公式如下</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="xva.jpg" alt="初始化公式" title="">
                </div>
                <div class="image-caption">初始化公式</div>
            </figure>

<p>这个方法实现起来比较简单。只需要在初始化<code>W</code>的时候乘上一个因子即可，常用的激活函数是<code>Relu</code>函数，<code>tanh</code>函数与其类似，因此只实现<code>Relu</code>函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">initialize_parameters_he</span>(<span class="params">layers_dims</span>):</span><br><span class="line"></span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = <span class="built_in">len</span>(layers_dims) - <span class="number">1</span> </span><br><span class="line">     </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, L + <span class="number">1</span>):</span><br><span class="line">        parameters[<span class="string">&#x27;W&#x27;</span> + <span class="built_in">str</span>(l)] = np.random.randn(layers_dims[l], layers_dims[l - <span class="number">1</span>]) * np.sqrt(<span class="number">2</span> / layers_dims[l - <span class="number">1</span>])</span><br><span class="line">        parameters[<span class="string">&#x27;b&#x27;</span> + <span class="built_in">str</span>(l)] = np.zeros((layers_dims[l], <span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>

<h1 id="三、梯度检验"><a href="#三、梯度检验" class="headerlink" title="三、梯度检验"></a>三、梯度检验</h1><p>梯度检验的目的是确认反向传播的正确性，因为反向传播较为复杂，很容易出现不知名bug。梯度检验的流程主要如下：</p>
<ol>
<li>将<code>W</code>和<code>b</code>合成一个新的变量<code>θ</code></li>
<li>利用公式计算<code>θ</code>近似值<br>  <img src="/approx.jpg" alt="近似值计算公式"></li>
<li>根据反向传播计算<code>W</code>和<code>b</code>并合成为新的的变量<code>θ</code></li>
<li>计算<code>θ</code>近似值与真实值之间的差距，如果小于某一个数，则认为计算正确，否则就需要检验所有的反向计算是否出错<br>  <img src="/diff.jpg" alt="差距计算公式"></li>
</ol>
<p>由于梯度检验的复杂性，本文实现的是较为简单的梯度检验算法<code>J=θx</code>。需要注意的是，梯度检验只能用于debug，不能用于训练，因为梯度检验需要的时间很长。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gradient_check</span>(<span class="params">x, theta, epsilon = <span class="number">1e-7</span></span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算近似值</span></span><br><span class="line">    thetaplus = theta + epsilon                             </span><br><span class="line">    thetaminus = theta - epsilon</span><br><span class="line">    J_plus = x * thetaplus</span><br><span class="line">    J_minus = x * thetaminus</span><br><span class="line">    gradapprox = (J_plus - J_minus) / (<span class="number">2</span> * epsilon)</span><br><span class="line">    </span><br><span class="line">    grad = x</span><br><span class="line"></span><br><span class="line">    numerator = np.linalg.norm(grad - gradapprox)</span><br><span class="line">    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)       </span><br><span class="line">    difference = numerator / denominator</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> difference &lt; <span class="number">1e-7</span>:</span><br><span class="line">        <span class="built_in">print</span> (<span class="string">&quot;The gradient is correct!&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span> (<span class="string">&quot;The gradient is wrong!&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> difference</span><br></pre></td></tr></table></figure>

<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>这周的课程主要是对神经网络的优化，包括Xavier初始化，以及如何debug等，以加快机器学习的速度，减少错误率。下周将学习全新的优化算法——MiniBatch梯度下降法。</p>

        </div>

        <blockquote class="post-copyright">
    
    <div class="content">
        
<span class="post-time">
    最后更新时间：<time datetime="2019-06-14T14:54:54.995Z" itemprop="dateUpdated">2019-06-14 22:54:54</time>
</span><br>


        
        原文链接：<a href="<%- url_for(page.path).replace(/index\.html$/, '') %>" target="_blank" rel="external"><%- page.permalink.replace(/index\.html$/, '') %></a>
        
    </div>
    
    <footer>
        <a href="https://KingsFish.github.io">
            <img src="/img/logo.jpg" alt="Fish">
            Fish
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            

            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://kingsfish.github.io/2017/09/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B504-%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96%E4%BB%A5%E5%8F%8A%E6%AD%A3%E5%88%99%E5%8C%96/&title=《深度学习实践04-参数优化以及正则化》 — 葫芦籽的碎碎念&pic=https://KingsFish.github.io/img/logo.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://kingsfish.github.io/2017/09/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B504-%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96%E4%BB%A5%E5%8F%8A%E6%AD%A3%E5%88%99%E5%8C%96/&title=《深度学习实践04-参数优化以及正则化》 — 葫芦籽的碎碎念&source=talk is cheap, show me the code" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://kingsfish.github.io/2017/09/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B504-%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96%E4%BB%A5%E5%8F%8A%E6%AD%A3%E5%88%99%E5%8C%96/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《深度学习实践04-参数优化以及正则化》 — 葫芦籽的碎碎念&url=https://kingsfish.github.io/2017/09/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B504-%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96%E4%BB%A5%E5%8F%8A%E6%AD%A3%E5%88%99%E5%8C%96/&via=https://KingsFish.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://kingsfish.github.io/2017/09/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B504-%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96%E4%BB%A5%E5%8F%8A%E6%AD%A3%E5%88%99%E5%8C%96/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/2017/09/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B505-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">深度学习实践05-优化算法</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/2017/09/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B503-%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">深度学习实践03-构建深层神经网络</h4>
      </a>
    </div>
  
</nav>



    




















</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        打赏点饭钱~
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/img/wechat.jpg" alt="打赏二维码">
        </div>
        
        <label class="reward-toggle">
            <input id="rewardToggle" type="checkbox" class="reward-toggle-check"
                data-wechat="/img/wechat.jpg" data-alipay="/img/alipay.jpg">
            <div class="reward-toggle-ctrol">
                <span class="reward-toggle-item wechat">微信</span>
                <span class="reward-toggle-label"></span>
                <span class="reward-toggle-item alipay">支付宝</span>
            </div>
        </label>
        
    </div>
</div>



</div>

        <footer class="footer">
    <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        站点总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        站点总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>博客内容遵循 <a rel="license noopener" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">知识共享 署名 - 非商业性 - 相同方式共享 4.0 国际协议</a></span>
        </p>
    </div>
    <div class="bottom">
        <p><span>Fish &copy; 2017 - 2023</span>
            <span>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://kingsfish.github.io/2017/09/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B504-%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96%E4%BB%A5%E5%8F%8A%E6%AD%A3%E5%88%99%E5%8C%96/&title=《深度学习实践04-参数优化以及正则化》 — 葫芦籽的碎碎念&pic=https://KingsFish.github.io/img/logo.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://kingsfish.github.io/2017/09/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B504-%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96%E4%BB%A5%E5%8F%8A%E6%AD%A3%E5%88%99%E5%8C%96/&title=《深度学习实践04-参数优化以及正则化》 — 葫芦籽的碎碎念&source=talk is cheap, show me the code" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://kingsfish.github.io/2017/09/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B504-%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96%E4%BB%A5%E5%8F%8A%E6%AD%A3%E5%88%99%E5%8C%96/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《深度学习实践04-参数优化以及正则化》 — 葫芦籽的碎碎念&url=https://kingsfish.github.io/2017/09/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B504-%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96%E4%BB%A5%E5%8F%8A%E6%AD%A3%E5%88%99%E5%8C%96/&via=https://KingsFish.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://kingsfish.github.io/2017/09/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B504-%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96%E4%BB%A5%E5%8F%8A%E6%AD%A3%E5%88%99%E5%8C%96/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAT4AAAE+CAAAAAAxUyPsAAAFOklEQVR42u3bwW7iQBAEUP7/p3cVKYddRTZV3eNceJyiADZ+E4ly9eT1ih9/Lh7/Ppsc4er1ybP3v/95hPvPfH+Eww98+PDhw4cvoLn6QPlF3r8rv+DNsrXL0C4APnz48OHDN+PbRI1ZxLm/+HYhk4WZHeHNVePDhw8fPnwP891/wSeB4P4ss1v9TbTKlxYfPnz48OH7fb7797a4+7r/vmqflfX48OHDhw/f03ybWjwvCNrBQHKEzULuB+348OHDhw9fy5cPmD/n50f29+HDhw8fvo/n228L22z2So6Q1xmz8DRz+H4vPnz48OHDN9EoivJ9FEi49zlhVsfPHPDhw4cPH77cYVOmb0r8NojMIk6L20YifPjw4cOHr+U7tSltj3UfYnKINvTMYtnlufDhw4cPH74R36bgzkPJrHyf0bQLn+Piw4cPHz58OV9blOfV+T6szKJMW9bXpcBmzoAPHz58+D6eL7+8dpDcPpuHj3ZJ2rohr/Xx4cOHDx++U3zt5rB8vN1W5PtR/aYUKM6IDx8+fPjwxWc8dcGnNn61lzob1Z+qHvDhw4cPH7493368PQsE7eW1t/r5uYrtcfjw4cOHD1/JNyu721J7NpKfBaO2wmgHA9GHxocPHz58+C6+bfcXNqu5NyXC2RjUDvi/f8aHDx8+fPhKvjy43IeDzQg8iR2v+NECzWqOOvfhw4cPHz58JdPskVf8RS1evvLslf53BHz48OHDh2/EtxkSb6qBtmSfDdHzAUMSzqKshw8fPnz48JVlwf7nfErfLuHs9y1iMZLHhw8fPnz4Yr7NDfksguRVeHupbYkwqzkuKwN8+PDhw4cv/jyzwrodM7eBZj8Ub6NVC4oPHz58+PDN+Galeb6hrd361pb17Q1/O0of/g3iw4cPHz58F3e7syJ+E1D2o/eWZnO9b8p6fPjw4cOHL+abBZc2jsy2i81oWvQ8MB3Lffjw4cOH7+P5ZmX9PtZsRtrt+Hw/GCgiCz58+PDhw3dx/Lw0bwfkT4ebWUDJRw7F3x0+fPjw4cM34muH07P4kp89jyA5bnKuYhnw4cOHDx+++Tf46l+IN1GjfdemUJgdOToaPnz48OHDV054T23eaouAfZhof5PzvYlQ+PDhw4cP31G+fbxIOPIC/dQi/VJwwYcPHz58+C5emRff+zCxKevbCHUq7rxRwocPHz58+GK+WR3fBpp2/JycKzlLO0qvhwr48OHDhw/faB6d39jPyoL2A7WLutlYto8++PDhw4cP34xvFgXa2/I8GO3PMhsV1AEIHz58+PDhG/G1IeDURrSng0i7qC0oPnz48OHDt+fbRIr2NbNzzWJKXtYXy4YPHz58+PDFfEmAyC9vVsTn78qH2VHgaMfhP8+CDx8+fPjwLfhmdXlyeS1NO56flQgJX72/AB8+fPjw4Yvv32fD8rYaOMXaFvr7IcEbK3z48OHDhy84b3v62eC8DQrRao+2xCWfqhg54MOHDx8+fCO+9us8Dyhtff8qHznEDC4KLvjw4cOHD9+Cr72ZT+qD9qPn521H6e0yRy0LPnz48OHDN/+Ojg7dnj4ZdT8xGGjfu18qfPjw4cOHL+HbDK1na5KXCDlTu0EtHz+8eRYfPnz48OEb8eVf5O27hkPoxQLMBv+rOgAfPnz48OH7Fb52Q9smIrTlQnuu1cQDHz58+PDhe4xvc8FtEsiXIY9E7QD+cHDBhw8fPnz44rJ+Pyw/dUufj8BnAeiRMTk+fPjw4cN3qJHexJr28mbLeaqMGBb9+PDhw4cP39fjL9lQ7v4xIoiBAAAAAElFTkSuQmCC" alt="微信分享二维码">
</div>




    <script src="//cdnjs.cloudflare.com/ajax/libs/node-waves/0.7.6/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };


</script>

<script src="/js/main.js?v=1.7.2"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="/js/search.js?v=1.7.2" async></script>






<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>



<script>
(function() {
    var OriginTitile = document.title, titleTime;
    document.addEventListener('visibilitychange', function() {
        if (document.hidden) {
            document.title = '死鬼去哪里了！';
            clearTimeout(titleTime);
        } else {
            document.title = '(つェ⊂)咦!又好了!';
            titleTime = setTimeout(function() {
                document.title = OriginTitile;
            },2000);
        }
    });
})();
</script>



</body>
</html>
